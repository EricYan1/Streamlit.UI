import streamlit as st
import time
import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
import matplotlib.pyplot as plt

# # åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹
# model = GaussianNB()
# # æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ
# data = np.load('E:\python\day02\MSIST.\mnist.npz',allow_pickle=True)
# x_train, y_train = data['x_train'], data['y_train']
# x_test, y_test = data['x_test'], data['y_test']
# model.fit(X = x_train, y = y_train)

# ChatGLM3å¤§æ¨¡å‹åŠ è½½
# from transformers import AutoTokenizer
# from transformers import AutoModel

# å…¨å±€è®¾ç½®
st.set_page_config(page_title = "ä¹³è…ºç–¾ç—…åˆ†ç±»æ¨¡å‹" , 
                   page_icon = "ğŸ‘©â€âš•ï¸",
                   layout = "wide",
                   initial_sidebar_state = "expanded")
# iconç½‘å€https://getemoji.com/

# ChatGLM3å¤§æ¨¡å‹åŠ è½½
# MODEL_PATH = "E:\python\day02"
# @st.cache_resource
# def load_model(MODEL_PATH):
#     tokennizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = MODEL_PATH,
#                                                trust_remote_code = True)
#     model = AutoModel.from_pretrained(pretrained_model_name_or_path = MODEL_PATH,
#                                       trust_remote_code = True,
#                                       device_map = True).eval()
#     return tokennizer,model
# tokennizer,model = load_model(MODEL_PATH = MODEL_PATH)

# ä¾§è¾¹æ å¸ƒå±€
with st.sidebar:
    st.markdown(body = "## ä¹³è…ºç–¾ç—…åˆ†ç±»æ¨¡å‹")
    st.divider()
    st.write("**æ¨¡å‹ä¿¡æ¯**ï¼šåŸºäºæœºå™¨å­¦ä¹ æ„å»ºä¹³è…ºç–¾ç—…åˆ†ç±»æ¨¡å‹ï¼Œç”¨äºä¹³è…ºåŒ»å­¦å›¾ç‰‡çš„è‡ªåŠ¨åˆ†ç±»")
    # st.divider()
    st.write("**ä¹³è…ºç–¾ç—…åˆ†ç±»**ï¼š")
    st.write("(1)ä¹³è…ºç‚ç—‡ï¼šåŒ…æ‹¬å“ºä¹³æœŸçš„ä¹³è…ºç‚ã€éå“ºä¹³æœŸçš„ä¹³è…ºç‚ï¼Œéå“ºä¹³æœŸçš„ä¹³è…ºç‚åˆåŒ…æ‹¬è‚‰èŠ½è‚¿æ€§å°å¶æ€§ä¹³è…ºç‚ã€æµ†ç»†èƒæ€§ä¹³è…ºç‚ç­‰ã€‚")
    st.write("(2)ä¹³è…ºè‚¿ç˜¤ï¼šåŒ…æ‹¬è‰¯æ€§è‚¿ç˜¤ã€æ¶æ€§è‚¿ç˜¤ï¼Œå…¶ä¸­è‰¯æ€§è‚¿ç˜¤ä¸­åŒ…æ‹¬ä¹³è…ºçº¤ç»´ç˜¤ã€ä¹³è…ºå¢ç”Ÿæ€§ç»“èŠ‚ã€ä¹³æˆ¿å›Šè‚¿ç­‰ï¼Œæ¶æ€§è‚¿ç˜¤åŒ…æ‹¬ä¹³è…ºç™Œä»¥åŠä¹³æˆ¿è‚‰ç˜¤ã€‚")
    st.write("(3)ä¹³è…ºå¢ç”Ÿï¼šæ—¢ä¸å±äºç‚ç—‡ï¼Œä¹Ÿä¸å±äºè‚¿ç˜¤ï¼Œæ˜¯ä¹³è…ºç»„ç»‡å¢ç”Ÿä»¥åŠä¹³è…ºç»„ç»‡ä¿®å¤ä¸å…¨çš„ä¸€ç§ç—…ç—‡ã€‚")
    st.divider()
    st.write("ä½œè€…AI4904")
    st.write(time.ctime())

# ä¸»æ˜¾ç¤ºåŒºå¸ƒå±€
st.title('ğŸ‘©â€âš•ï¸ ä¹³è…ºç–¾ç—…åˆ†ç±» ğŸ‘©â€âš•ï¸')
st.divider()

# åˆ—å¼å¸ƒå±€
col1, col2, col3 = st.columns(3)
with col1:
    st.write("**å›¾ç‰‡ä¸€**")
    # ä¸Šä¼ å›¾ç‰‡
    image1 = st.file_uploader("ä¸Šä¼ å›¾ç‰‡1", type = ["jpg", "png"])
    if image1:
    # è¯»å–å›¾ç‰‡
        img = st.image(image1, use_column_width = True)
    # ä¿å­˜å›¾ç‰‡
        with open("img.jpg", "wb") as f:
            f.write(image1.read())
    st.divider()
    # # è¾“å…¥å›¾ç‰‡1è¿›è¡Œé¢„æµ‹
    # out1 = model.predict(image1)
    # è¾“å‡ºå›¾ç‰‡1çš„åˆ†ç±»ç»“æœ
    text1 = st.write("**å›¾ç‰‡ä¸€ç»“æœä¸ºï¼š**")
    # text1 = st.write("**å›¾ç‰‡ä¸€ç»“æœä¸ºï¼š**", out1)
    st.divider()

with col2:
    st.write("**å›¾ç‰‡äºŒ**")
    # ä¸Šä¼ å›¾ç‰‡
    image2 = st.file_uploader("ä¸Šä¼ å›¾ç‰‡2", type = ["jpg", "png"])
    if image2:
    # è¯»å–å›¾ç‰‡
        img = st.image(image2, use_column_width = True)
    # ä¿å­˜å›¾ç‰‡
        with open("img.jpg", "wb") as f:
            f.write(image2.read())
    st.divider()
    # # è¾“å…¥å›¾ç‰‡2è¿›è¡Œé¢„æµ‹
    # out2 = model.predict(image2)
    # è¾“å‡ºå›¾ç‰‡1çš„åˆ†ç±»ç»“æœ
    text2 = st.write("**å›¾ç‰‡äºŒç»“æœä¸ºï¼š**")
    # text2 = st.write("**å›¾ç‰‡äºŒç»“æœä¸ºï¼š**", out2)
    st.divider()

with col3:
    st.write("**å›¾ç‰‡ä¸‰**")
    # ä¸Šä¼ å›¾ç‰‡
    image3 = st.file_uploader("ä¸Šä¼ å›¾ç‰‡3", type = ["jpg", "png"])
    if image3:
    # è¯»å–å›¾ç‰‡
        img = st.image(image3, use_column_width = True)
    # ä¿å­˜å›¾ç‰‡
        with open("img.jpg", "wb") as f:
            f.write(image3.read())
    st.divider()
    # # è¾“å…¥å›¾ç‰‡3è¿›è¡Œé¢„æµ‹
    # out3 = model.predict(image3)
    # è¾“å‡ºå›¾ç‰‡3çš„åˆ†ç±»ç»“æœ
    text3 = st.write("**å›¾ç‰‡ä¸‰ç»“æœä¸ºï¼š**")
    # text3 = st.write("**å›¾ç‰‡ä¸‰ç»“æœä¸ºï¼š**", out3)
    st.divider()

# èŠå¤©è¾“å…¥æ¡†(é»˜è®¤åœ¨æœ€ä¸‹é¢)
query = st.chat_input(placeholder = "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜......")
# å¦‚æœæœ‰è¾“å…¥å†…å®¹
if query:
    # ç”¨æˆ·æé—®æ¸²æŸ“
    with st.chat_message(name = "user"):
        st.markdown(query)

     # é¢„è®¾å›ç­”æ¸²æŸ“
    with st.chat_message(name = "assistant"):
        out = st.empty()
        txt = ""
        text = "å»ºç«‹è‰¯å¥½çš„ç”Ÿæ´»æ–¹å¼ï¼Œè°ƒæ•´å¥½ç”Ÿæ´»èŠ‚å¥ï¼Œä¿æŒå¿ƒæƒ…èˆ’ç•…ã€‚åšæŒä½“è‚²é”»ç‚¼ï¼Œç§¯æå‚åŠ ç¤¾äº¤æ´»åŠ¨ï¼Œé¿å…å’Œå‡å°‘ç²¾ç¥ã€å¿ƒç†ç´§å¼ å› ç´ "
        for i in text:
            txt += str(i)
            out.markdown(txt)
            time.sleep(0.1)

    # ChatGLM3å¤§æ¨¡å‹å›ç­”æ¸²æŸ“
    # with st.chat_message(name = "assistant"):
    #     out = st.empty()
    #     for response,history in model.stream_chat(tokenizer=tokenizer.
    #                                               model=model,
    #                                               query=query):
    #     out.markdown(response)
    

